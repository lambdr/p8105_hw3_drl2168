---
title: 'P8105: Homework 3'
author: "Derek Lamb"
date: "`r Sys.Date()`"
output: github_document
---

I will load all necessary packages here. I also am including the code that Dr. Goldsmith showed in class, to set figure color and size preferences. I have modified it slightly: I prefer `theme_bw()` over `theme_minimal()` and I may change the color scheme from the default `viridis`.
```{r load packages, message = FALSE}
library(tidyverse)
library(knitr)

opts_chunk$set(
  fig.width = 6,
  out.width = "90%"
)

## These are options that i'm going to treat as default for now
## I don't love viridis, but don't have a better option yet
theme_set(theme_bw() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1
```{r instacart data import}
# Load data from the provided package
library(p8105.datasets)
data("instacart")
```

### Data set size, user behavior
This data set is massive, containing `r nrow(instacart)` observations of `r ncol(instacart)` variables. Each row is a specific item from an order, in total `r instacart |> pull(order_id) |> unique() |> length()` orders are included from `r instacart |> pull(user_id) |> unique() |> length()` unique users. There appear to be no repeat orders from users in this data set, although `r instacart |> pull(reordered) |> sum()` items are items that were previously ordered by that user. This accounts for `r (instacart |> pull(reordered) |> sum()/nrow(instacart)) |> scales::percent()` of the orders, showing that only a small portion of instacart activity is captured by this data set. There are no first-time orders in this data set; this is on average order number `r instacart |> pull(order_number) |> mean() |> round(digits = 0)` for users (median order number `r instacart |> pull(order_number) |> median()` ). On average, users placed their orders `r instacart |> pull(days_since_prior_order) |> mean() |> round(digits = 1)` days after their previous one.

### Aisles & products
There are `r instacart |> pull(aisle_id) |> unique() |> length()` unique aisles included in the data set. In the following code chunk, I'll find the most ordered from aisles.
```{r aisle orders}
instacart |> 
  group_by(aisle) |> 
  summarize(n_obs = n()) |> 
  arrange(desc(n_obs)) |> 
  head() |> 
  kable(col.names = c("Aisle", "Number of Orders"))
```
The two most ordered from aisles are fresh vegetables and fresh fruits, with about 150,000 orders each. Below, you can see a plot of all aisles with more than 10,000 orders.

```{r aisle plot}
instacart |> 
  group_by(aisle) |> 
  summarize(n_obs = n()) |> 
  filter(n_obs >=10000) |> 
  ggplot(aes(y = reorder(aisle, n_obs), x = n_obs)) +
  geom_bar(alpha = .9, stat = "identity") + 
  labs(
    title = "Instacart aisles with more than 10,000 orders",
    x = "Number of orders",
    y = "Aisle"
  )
```

To look more closely at the data set, I created a table examining the three most popular items from a set of aisles.
```{r most pop items in 3 rows, message = FALSE}
instacart |> 
  group_by(aisle, product_name) |> 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") |> 
  summarize(n_obs = n()) |> 
  filter(min_rank(desc(n_obs)) <= 3) |> 
  mutate(
    rank = min_rank(desc(n_obs)),
    item = paste(product_name, " (", as.character(n_obs),")", sep = "")
         ) |> 
  select(-product_name, -n_obs) |> 
  pivot_wider(
    names_from = aisle,
    values_from = item
  ) |> 
  arrange(rank) |> 
  kable(col.names = c("Rank in Aisle", "Baking Ingredients", "Dog Food & Care", "Packaged Vegatables & Fruits"), caption = "Item (# of orders)")
```

### Apples & Ice Cream
```{r pink lady}
instacart |> 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") |> 
  group_by(product_name, order_dow) |> 
  summarize(avg_hour = mean(order_hour_of_day)) |> 
  pivot_wider(
    names_from = order_dow,
    values_from = avg_hour
  ) |> 
  kable(digits = 1, col.names = c("Product", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
```

While the majority of ordering appears concentrated in the midle of the day, coffee ice cream is bought a bit later in the afternoon, particularly during Weekdays.


# Problem 2
In the code chunk below, I will load the data for this problem, filter `topic` to just Overall Health, and recode `response` as a factor variable. 
```{r brfss data import}
data("brfss_smart2010") 
df_brfss = brfss_smart2010|> 
  janitor::clean_names() |>
  filter(topic == "Overall Health")|> 
  mutate(response = factor(response, levels=c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

### Number of sites
I will look at the number of sites per state.
```{r number of states 2002}
df_brfss |> 
  filter(year == 2002) |> 
  group_by(locationabbr) |> 
  summarize(n_site = n_distinct(locationdesc)) |> 
  filter(n_site >= 7) |> 
  pull(locationabbr)

```

These 6 states have 7 or more locations sampled within them in 2002, whereas there were 14 states that had 7 or more locations in 2010:

```{r number of states 2010}
df_brfss |> 
  filter(year == 2010) |> 
  group_by(locationabbr) |> 
  summarize(n_site = n_distinct(locationdesc)) |> 
  filter(n_site >= 7) |> 
  pull(locationabbr)

```

### Excellent

